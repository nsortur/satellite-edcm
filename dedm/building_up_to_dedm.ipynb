{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch_geometric.utils\n",
    "from e3nn import o3\n",
    "from e3nn import nn as enn\n",
    "from e3nn.math import soft_one_hot_linspace\n",
    "from e3nn.nn import Gate\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "import rem.e3nn_utils as e3nn_utils\n",
    "from e3nn.nn import SO3Activation\n",
    "import rem.pooling as pooling\n",
    "\n",
    "from datasets import DragMeshDataset\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelsortur/Documents/codestuff/sat-modeling/satellite-edcm/datasets.py:54: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(utils.to_absolute_path(attr_file), delim_whitespace=True, header=None)\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "ds = DragMeshDataset(\"data/cube50k.dat\", \"STLs/Cube_38_1m.stl\", return_features_separately=False)\n",
    "dl = iter(DataLoader(ds, batch_size=1, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    irreps_node_input,\n",
    "    irreps_node_output,\n",
    "    max_radius,\n",
    "    mul=50,\n",
    "    layers=3,\n",
    "    lmax=2,\n",
    "    pool_nodes=True,\n",
    "  ) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    self.lmax = lmax\n",
    "    self.max_radius = max_radius\n",
    "    self.number_of_basis = 10\n",
    "    self.pool_nodes = pool_nodes\n",
    "\n",
    "    irreps_node_hiddens = list()\n",
    "    irreps_edge_hiddens = list()\n",
    "    for layer_lmax in lmax[:-1]:\n",
    "      irreps_node_hiddens.append(o3.Irreps(\n",
    "        #[(mul * 2 * l + 1, (l, 1)) for l in range(layer_lmax + 1)]\n",
    "        [(mul, (l, p)) for l in range(layer_lmax + 1) for p in [-1, 1]]\n",
    "      ))\n",
    "      irreps_edge_hiddens.append(o3.Irreps.spherical_harmonics(layer_lmax))\n",
    "\n",
    "\n",
    "    self.irreps_node_seq = [irreps_node_input] + irreps_node_hiddens + [irreps_node_output]\n",
    "    irreps_edge_seq = irreps_edge_hiddens + [o3.Irreps.spherical_harmonics(lmax[-1])]\n",
    "    self.mp = MessagePassing(\n",
    "      irreps_node_sequence=self.irreps_node_seq,\n",
    "      irreps_edge_attrs=irreps_edge_seq,\n",
    "      fc_neurons=[self.number_of_basis, 100],\n",
    "      max_radius=max_radius,\n",
    "      lmax=self.lmax\n",
    "    )\n",
    "    self.irreps_node_input = self.mp.irreps_node_input\n",
    "    self.irreps_node_output = self.mp.irreps_node_output\n",
    "\n",
    "  def forward(self, data: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    batch, node_outputs = self.mp(data)\n",
    "\n",
    "    if self.pool_nodes:\n",
    "      return torch_geometric.utils.scatter(node_outputs, batch, dim=0, reduce='mean')\n",
    "    else:\n",
    "      return node_outputs\n",
    "\n",
    "class MessagePassing(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    irreps_node_sequence,\n",
    "    irreps_edge_attrs,\n",
    "    fc_neurons,\n",
    "    max_radius,\n",
    "    lmax,\n",
    "  ) -> None:\n",
    "    super().__init__()\n",
    "    self.lmax = lmax\n",
    "    self.max_radius = max_radius\n",
    "    self.number_of_basis = 10\n",
    "\n",
    "    irreps_node_sequence = [o3.Irreps(irreps) for irreps in irreps_node_sequence]\n",
    "    self.irreps_edge_attrs = [o3.Irreps(irreps) for irreps in irreps_edge_attrs]\n",
    "\n",
    "    act = {\n",
    "      1: torch.nn.functional.silu,\n",
    "      -1: torch.tanh,\n",
    "    }\n",
    "    act_gates = {\n",
    "      1: torch.sigmoid,\n",
    "      -1: torch.tanh,\n",
    "    }\n",
    "\n",
    "    self.layers = torch.nn.ModuleList()\n",
    "\n",
    "    self.irreps_node_sequence = [irreps_node_sequence[0]]\n",
    "    irreps_node = irreps_node_sequence[0]\n",
    "\n",
    "    for li, (irreps_node_hidden, irreps_edge_attr) in enumerate(zip(irreps_node_sequence[1:-1], self.irreps_edge_attrs[:-1])):\n",
    "      irreps_scalars = o3.Irreps(\n",
    "        [\n",
    "          (mul, ir)\n",
    "          for mul, ir in irreps_node_hidden\n",
    "          if ir.l == 0\n",
    "          and e3nn_utils.tp_path_exists(\n",
    "            irreps_node, irreps_edge_attr, ir\n",
    "          )\n",
    "        ]\n",
    "      ).simplify()\n",
    "      irreps_gated = o3.Irreps(\n",
    "        [\n",
    "          (mul, ir)\n",
    "          for mul, ir in irreps_node_hidden\n",
    "          if ir.l > 0\n",
    "          and e3nn_utils.tp_path_exists(\n",
    "            irreps_node, irreps_edge_attr, ir\n",
    "          )\n",
    "        ]\n",
    "      )\n",
    "      if irreps_gated.dim > 0:\n",
    "        if e3nn_utils.tp_path_exists(irreps_node, irreps_edge_attr, \"0e\"):\n",
    "          ir = \"0e\"\n",
    "        elif e3nn_utils.tp_path_exists(\n",
    "          irreps_node, irreps_edge_attr, \"0o\"\n",
    "        ):\n",
    "          ir = \"0o\"\n",
    "        else:\n",
    "          raise ValueError(\n",
    "            f\"irreps_node={irreps_node} times irreps_edge_attr={self.irreps_edge_attr} is unable to produce gates \"\n",
    "            f\"needed for irreps_gated={irreps_gated}\"\n",
    "          )\n",
    "      else:\n",
    "        ir = None\n",
    "      irreps_gates = o3.Irreps([(mul, ir) for mul, _ in irreps_gated]).simplify()\n",
    "\n",
    "      gate = Gate(\n",
    "        irreps_scalars,\n",
    "        [act[ir.p] for _, ir in irreps_scalars],  # scalar\n",
    "        irreps_gates,\n",
    "        [act_gates[ir.p] for _, ir in irreps_gates],  # gates (scalars)\n",
    "        irreps_gated,  # gated tensors\n",
    "      )\n",
    "      conv = GraphConvolution(\n",
    "        irreps_node,\n",
    "        irreps_edge_attr,\n",
    "        gate.irreps_in,\n",
    "        fc_neurons,\n",
    "      )\n",
    "      #if li == 1:\n",
    "      #  pool = pooling.VoxelPooling(gate.irreps_out, self.lmax, [0.5, 0.5, 0.5])#, start=[-2, -2, -2],end=[2, 2, 2])\n",
    "      #else:\n",
    "      #  pool = pooling.VoxelPooling(gate.irreps_out, self.lmax, [0.1, 0.1, 0.5])#, start=[-2, -2, -2],end=[2, 2, 2])\n",
    "      #pool = pooling.EdgePooling(gate.irreps_out, self.lmax)\n",
    "      pool = pooling.TopKPooling(gate.irreps_out, self.lmax)\n",
    "      #pool = None\n",
    "      self.layers.append(e3nn_utils.GraphConvBlock(conv, gate, pool))\n",
    "      irreps_node = gate.irreps_out\n",
    "      self.irreps_node_sequence.append(irreps_node)\n",
    "\n",
    "    irreps_node_output = irreps_node_sequence[-1]\n",
    "    self.layers.append(\n",
    "      e3nn_utils.GraphConvBlock(\n",
    "        GraphConvolution(\n",
    "          irreps_node,\n",
    "          self.irreps_edge_attrs[-1],\n",
    "          irreps_node_output,\n",
    "          fc_neurons,\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    self.irreps_node_sequence.append(irreps_node_output)\n",
    "\n",
    "    self.irreps_node_input = self.irreps_node_sequence[0]\n",
    "    self.irreps_node_output = self.irreps_node_sequence[-1]\n",
    "\n",
    "  def forward(self, data) -> torch.Tensor:\n",
    "    for i, lay in enumerate(self.layers):\n",
    "      # Edge attributes\n",
    "      edge_sh = o3.spherical_harmonics(\n",
    "        range(self.lmax[i] + 1), data.edge_vec, True, normalization=\"component\"\n",
    "      )\n",
    "      data.edge_attr_sh = edge_sh\n",
    "\n",
    "      # Edge length embedding\n",
    "      edge_length = data.edge_vec.norm(dim=1)\n",
    "      data.edge_scalars = soft_one_hot_linspace(\n",
    "        edge_length,\n",
    "        0.0,\n",
    "        self.max_radius,\n",
    "        self.number_of_basis,\n",
    "        basis=\"smooth_finite\",  # the smooth_finite basis with cutoff = True goes to zero at max_radius\n",
    "        cutoff=True,  # no need for an additional smooth cutoff\n",
    "      ).mul(self.number_of_basis**0.5)\n",
    "\n",
    "      # Forward\n",
    "      data = lay(data)\n",
    "\n",
    "    return data.batch, data.x\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class GraphConvolution(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self, irreps_node_input, irreps_edge_attr, irreps_node_output, fc_neurons\n",
    "  ) -> None:\n",
    "    super().__init__()\n",
    "    self.irreps_node_input = o3.Irreps(irreps_node_input)\n",
    "    self.irreps_edge_attr = o3.Irreps(irreps_edge_attr)\n",
    "    self.irreps_node_output = o3.Irreps(irreps_node_output)\n",
    "    self.sc = o3.Linear(self.irreps_node_input, self.irreps_node_output)\n",
    "    # self.lin1 = o3.FullyConnectedTensorProduct(self.irreps_node_input, self.irreps_node_attr, self.irreps_node_input)\n",
    "\n",
    "    irreps_mid = []\n",
    "    instructions = []\n",
    "    for i, (mul, ir_in) in enumerate(self.irreps_node_input):\n",
    "      for j, (_, ir_edge) in enumerate(self.irreps_edge_attr):\n",
    "        for ir_out in ir_in * ir_edge:\n",
    "          if ir_out in self.irreps_node_output or ir_out == o3.Irrep(0, 1):\n",
    "            k = len(irreps_mid)\n",
    "            irreps_mid.append((mul, ir_out))\n",
    "            instructions.append((i, j, k, \"uvu\", True))\n",
    "    irreps_mid = o3.Irreps(irreps_mid)\n",
    "    irreps_mid, p, _ = irreps_mid.sort()\n",
    "\n",
    "    assert irreps_mid.dim > 0, (\n",
    "      f\"irreps_node_input={self.irreps_node_input} time irreps_edge_attr={self.irreps_edge_attr} produces nothing \"\n",
    "      f\"in irreps_node_output={self.irreps_node_output}\"\n",
    "    )\n",
    "    instructions = [(i_1, i_2, p[i_out], mode, train) for i_1, i_2, i_out, mode, train in instructions]\n",
    "\n",
    "    tp = o3.TensorProduct(\n",
    "      self.irreps_node_input,\n",
    "      self.irreps_edge_attr,\n",
    "      irreps_mid,\n",
    "      instructions,\n",
    "      internal_weights=False,\n",
    "      shared_weights=False,\n",
    "    )\n",
    "    self.fc = enn.FullyConnectedNet(fc_neurons + [tp.weight_numel], torch.nn.functional.silu)\n",
    "    self.tp = tp\n",
    "\n",
    "    self.lin = o3.Linear(irreps_mid, self.irreps_node_output)\n",
    "    # self.lin2 = o3.FullyConnectedTensorProduct(...)\n",
    "\n",
    "    # inspired by https://arxiv.org/pdf/2002.10444.pdf\n",
    "    self.alpha = o3.Linear(irreps_mid, \"0e\")\n",
    "    with torch.no_grad():\n",
    "      self.alpha.weight.zero_()\n",
    "\n",
    "  def forward(self, data) -> torch.Tensor:\n",
    "    weight = self.fc(data.edge_scalars)\n",
    "\n",
    "    node_self_connection =  self.sc(data.x)\n",
    "    # print(f\"Node self connection: {node_self_connection.shape}\")\n",
    "    # print(node_self_connection)\n",
    "    edge_features = self.tp(data.x[data.edge_index[0]], data.edge_attr_sh, weight)\n",
    "    # print(f\"Edge features: {edge_features.shape}\")\n",
    "    # print(edge_features)\n",
    "    node_features = torch_geometric.utils.scatter(edge_features, data.edge_index[1], dim=0, dim_size=data.x.shape[0], reduce='mean')\n",
    "    # print(f\"Node features: {node_features.shape}\")\n",
    "    # print(node_features)\n",
    "\n",
    "    alpha = self.alpha(node_features)\n",
    "    # print(f\"Alpha: {alpha.shape}\")\n",
    "    # print(alpha)\n",
    "    node_conv_out = self.lin(node_features)\n",
    "    # print(f\"Node conv out: {node_conv_out.shape}\")\n",
    "    # print(node_conv_out)\n",
    "\n",
    "    m = self.sc.output_mask\n",
    "    alpha = (1 - m) + alpha * m\n",
    "    return node_self_connection + alpha * node_conv_out\n",
    "  \n",
    "class Decoder(torch.nn.Module):\n",
    "  def __init__(self, lmax_in, lmax_out, f_in, f_out):\n",
    "    super().__init__()\n",
    "\n",
    "    grid_s2 = e3nn_utils.s2_near_identity_grid()\n",
    "    grid_so3 = e3nn_utils.so3_near_identity_grid()\n",
    "\n",
    "    self.so3_conv1 = e3nn_utils.SO3Convolution(\n",
    "      f_in, 64, lmax_in, kernel_grid=grid_so3\n",
    "    )\n",
    "    self.act1 = SO3Activation(lmax_in, lmax_out, torch.relu, resolution=12)\n",
    "\n",
    "    self.so3_conv2 = e3nn_utils.SO3Convolution(\n",
    "      64, 128, lmax_in, kernel_grid=grid_so3\n",
    "    )\n",
    "    self.act2 = SO3Activation(lmax_in, lmax_out, torch.relu, resolution=12)\n",
    "\n",
    "    self.so3_conv3 = e3nn_utils.SO3Convolution(\n",
    "      128, 256, lmax_in, kernel_grid=grid_so3\n",
    "    )\n",
    "\n",
    "    # 2) Project out ONLY the l=0 block (the scalars)\n",
    "      #    This activation sets lmax_out=0 => keep only l=0\n",
    "    self.act3 = SO3Activation(lmax_in, 0, torch.relu, resolution=12)\n",
    "    \n",
    "    self.lin = o3.Linear(\"1x0e\", f\"1x0e\", f_in=256, f_out=1)\n",
    "\n",
    "    # # Output: Maps to 53 (rho_0, rho_1, rho_2, rho_3, ...) -> 53 S2 signals\n",
    "    # if self.invariant_out:\n",
    "    #   self.act3 = SO3Activation(lmax_in, 0, torch.relu, resolution=12)\n",
    "    #   self.lin = o3.Linear(256, f_out)\n",
    "    # else:\n",
    "    #   self.act3 = SO3Activation(lmax_in, lmax_out, torch.relu, resolution=12)\n",
    "    #   self.lin = e3nn_utils.SO3ToS2Convolution(\n",
    "    #     256, f_out, lmax_out, kernel_grid=grid_s2\n",
    "    #   )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.so3_conv1(x)\n",
    "    x = self.act1(x)\n",
    "\n",
    "    x = self.so3_conv2(x)\n",
    "    x = self.act2(x)\n",
    "\n",
    "    x = self.so3_conv3(x)\n",
    "    x = self.act3(x)  # keep only l=0\n",
    "    print(x.shape)\n",
    "\n",
    "    x = self.lin(x)   # from 256 scalar channels -> f_out scalar channels\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEDM(torch.nn.Module):\n",
    "  def __init__(self, num_node_features, z_lmax, max_radius, out_dim):\n",
    "    super().__init__()\n",
    "\n",
    "#    z_lmax = 4\n",
    "    self.lmax = z_lmax\n",
    "    self.out_dim = out_dim\n",
    "    f = 16\n",
    "\n",
    "    self.irreps_in = o3.Irreps(f\"{num_node_features}x0e\")\n",
    "    self.irreps_latent = e3nn_utils.so3_irreps(z_lmax)\n",
    "    self.irreps_enc_out = o3.Irreps(\n",
    "      #[(f, (l, p)) for l in range((z_lmax // 2) + 1) for p in [-1,1]]\n",
    "      [(f, (l, p)) for l in range((z_lmax) + 1) for p in [-1,1]]\n",
    "    )\n",
    "    self.irreps_enc_out = o3.Irreps(\"16x0e\")\n",
    "    \n",
    "    self.encoder = GNN(\n",
    "        irreps_node_input=self.irreps_in,\n",
    "        irreps_node_output=self.irreps_enc_out,\n",
    "        max_radius=max_radius,\n",
    "        mul=f,\n",
    "        #lmax=[self.lmax // 2, self.lmax // 2, self.lmax // 2],\n",
    "        lmax=[self.lmax, self.lmax],\n",
    "      )\n",
    "\n",
    "    # TODO figure out what this linear layer actually is\n",
    "    # remove nonlinearities (could be an error) then VN could help\n",
    "    # equivariance error for encoder and decoder (on a layer by layer basis)\n",
    "    # overfit to a spherical signal in the decoder\n",
    "    # latent space\n",
    "    # TODO develop a baseline mesh to radar model and see what the error is\n",
    "    # resolution?\n",
    "    self.lin = o3.Linear(self.irreps_enc_out, self.irreps_latent, f_in=1, f_out=f)\n",
    "    self.decoder = Decoder(z_lmax, z_lmax, f, out_dim)\n",
    "\n",
    "  def forward(self, x, return_latent=False):\n",
    "    batch_size = x.batch.max() + 1\n",
    "    gnn_out = self.encoder(x)\n",
    "    z = self.lin(gnn_out.view(batch_size, 1, -1))\n",
    "    out = self.decoder(z)\n",
    "\n",
    "    return gnn_out, out\n",
    "    cartesian = self.ar2los(x.orientation)\n",
    "    out_response = self._getResponse(out, cartesian)\n",
    "\n",
    "    if return_latent:\n",
    "      return (out_response, out)\n",
    "    else:\n",
    "      return out_response\n",
    "\n",
    "  def _getResponse(self, out, pose):\n",
    "    if self.invariant_out:\n",
    "      return out\n",
    "    else:\n",
    "      sh = torch.concatenate(\n",
    "        [o3.spherical_harmonics(l, pose, True) for l in range(self.lmax + 1)], dim=1\n",
    "      ).unsqueeze(2)  # B x (L^2 * S^2) x 1\n",
    "      response = torch.bmm(out, sh).squeeze()  # B x D\n",
    "\n",
    "      return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/neelsortur/miniconda3/envs/satsims/lib/python3.11/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dedm = DEDM(num_node_features=5, z_lmax=4, max_radius=1.8, out_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[38, 5], edge_index=[2, 108], pos=[38, 3], edge_vec=[108, 3], orientation=[1, 2], batch=[38], ptr=[2])\n",
      "tensor([[0., 0.]])\n",
      "tensor([[0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993],\n",
      "        [0.6246, 0.9524, 0.0676, 0.6570, 0.0993]])\n",
      "tensor([[-5.0000e-01,  5.0000e-01,  0.0000e+00],\n",
      "        [-5.0000e-01,  5.0000e-01, -5.0000e-01],\n",
      "        [-5.0000e-01,  0.0000e+00, -5.0000e-01],\n",
      "        [-5.0000e-01,  1.4624e-01, -1.4639e-01],\n",
      "        [-5.0000e-01,  0.0000e+00,  5.0000e-01],\n",
      "        [-5.0000e-01, -1.5055e-01,  1.4442e-01],\n",
      "        [-5.0000e-01, -5.0000e-01,  0.0000e+00],\n",
      "        [-5.0000e-01, -5.0000e-01,  5.0000e-01],\n",
      "        [-5.0000e-01,  5.0000e-01,  5.0000e-01],\n",
      "        [-5.0000e-01, -9.8562e-04, -7.4565e-05],\n",
      "        [-5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
      "        [ 5.0000e-01,  5.0000e-01,  0.0000e+00],\n",
      "        [ 5.0000e-01,  1.4624e-01, -1.4639e-01],\n",
      "        [ 5.0000e-01,  0.0000e+00, -5.0000e-01],\n",
      "        [ 5.0000e-01,  5.0000e-01, -5.0000e-01],\n",
      "        [ 5.0000e-01,  0.0000e+00,  5.0000e-01],\n",
      "        [ 5.0000e-01, -5.0000e-01,  5.0000e-01],\n",
      "        [ 5.0000e-01, -5.0000e-01,  0.0000e+00],\n",
      "        [ 5.0000e-01, -1.5055e-01,  1.4442e-01],\n",
      "        [ 5.0000e-01,  5.0000e-01,  5.0000e-01],\n",
      "        [ 5.0000e-01, -9.8562e-04, -7.4565e-05],\n",
      "        [ 5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
      "        [ 0.0000e+00, -5.0000e-01,  5.0000e-01],\n",
      "        [-1.4639e-01, -5.0000e-01,  1.4624e-01],\n",
      "        [ 1.4442e-01, -5.0000e-01, -1.5055e-01],\n",
      "        [ 0.0000e+00, -5.0000e-01, -5.0000e-01],\n",
      "        [-7.4625e-05, -5.0000e-01, -9.8562e-04],\n",
      "        [ 0.0000e+00,  5.0000e-01,  5.0000e-01],\n",
      "        [-1.4639e-01,  5.0000e-01,  1.4624e-01],\n",
      "        [ 0.0000e+00,  5.0000e-01, -5.0000e-01],\n",
      "        [ 1.4442e-01,  5.0000e-01, -1.5055e-01],\n",
      "        [-7.4625e-05,  5.0000e-01, -9.8562e-04],\n",
      "        [ 1.4624e-01, -1.4639e-01, -5.0000e-01],\n",
      "        [-1.5055e-01,  1.4442e-01, -5.0000e-01],\n",
      "        [-9.8562e-04, -7.4565e-05, -5.0000e-01],\n",
      "        [ 1.4624e-01, -1.4639e-01,  5.0000e-01],\n",
      "        [-1.5055e-01,  1.4442e-01,  5.0000e-01],\n",
      "        [-9.8562e-04, -7.4565e-05,  5.0000e-01]])\n"
     ]
    }
   ],
   "source": [
    "samp, y = next(dl)\n",
    "samp_copy = samp.clone()\n",
    "samp_copy2 = samp.clone()\n",
    "samp.orientation = torch.tensor([[0.0, 0.0]])\n",
    "print(samp)\n",
    "print(samp.orientation)\n",
    "print(samp.x)\n",
    "print(samp.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1])\n",
      "torch.Size([1, 1, 1])\n",
      "tensor([[[-2.3762e-05]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.0499,  0.0266,  0.0319,  0.0059, -0.0337,  0.0068,  0.0067,  0.0191,\n",
      "         -0.0003, -0.0375, -0.0089, -0.0043, -0.0044,  0.0049,  0.0733, -0.0324]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ret = dedm(samp)\n",
    "print(ret[1].shape)\n",
    "print(ret[1])\n",
    "print(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1])\n",
      "torch.Size([1, 1, 1])\n",
      "tensor([[[-2.3762e-05]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.0499,  0.0266,  0.0319,  0.0059, -0.0337,  0.0068,  0.0067,  0.0191,\n",
      "         -0.0003, -0.0375, -0.0089, -0.0043, -0.0044,  0.0049,  0.0733, -0.0324]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "samp_copy.pos = samp_copy.pos @ o3.rand_matrix()\n",
    "ret_copy = dedm(samp_copy)\n",
    "print(ret_copy[1].shape)\n",
    "print(ret_copy[1])\n",
    "print(ret_copy[0])\n",
    "assert torch.allclose(ret_copy[0], ret[0], atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1])\n",
      "torch.Size([1, 1, 1])\n",
      "tensor([[[-2.3762e-05]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.0499,  0.0266,  0.0319,  0.0059, -0.0337,  0.0068,  0.0067,  0.0191,\n",
      "         -0.0003, -0.0375, -0.0089, -0.0043, -0.0044,  0.0049,  0.0733, -0.0324]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "noise = torch.randn_like(samp_copy2.pos) * 1.5  # small random noise shouldn't be equivariant\n",
    "samp_copy2.pos += noise\n",
    "ret_copy2 = dedm(samp_copy2)\n",
    "print(ret_copy2[1].shape)\n",
    "print(ret_copy2[1])\n",
    "print(ret_copy2[0])\n",
    "# ret_copy2[0] all close to ret[0] -> equivariant\n",
    "assert torch.allclose(ret_copy2[0], ret[0], atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satsims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
